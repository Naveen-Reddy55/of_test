{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "de4faf6e739549b39de4713fff901147": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86d680cc08c84d9f84aa7dcba6b65daa",
              "IPY_MODEL_4958b7adafd3427486dbf2c944e257fd",
              "IPY_MODEL_5706775750cf41878e3674389abf9758"
            ],
            "layout": "IPY_MODEL_29208aa6a7c24da598437704d45775e7"
          }
        },
        "86d680cc08c84d9f84aa7dcba6b65daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98a95d387c434bde94d31e85d4ba797a",
            "placeholder": "​",
            "style": "IPY_MODEL_b571a882de14415c97ea703e0f0393e5",
            "value": "100%"
          }
        },
        "4958b7adafd3427486dbf2c944e257fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fc1f977abce4edc914a86203fec425b",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c13b396bd774f24b5dba4fcf4e533f9",
            "value": 3
          }
        },
        "5706775750cf41878e3674389abf9758": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cafec088deaa4044acecbff44d11c516",
            "placeholder": "​",
            "style": "IPY_MODEL_6a1a3ac56b5345a19c79e6afe378ee38",
            "value": " 3/3 [00:00&lt;00:00, 81.91it/s]"
          }
        },
        "29208aa6a7c24da598437704d45775e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98a95d387c434bde94d31e85d4ba797a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b571a882de14415c97ea703e0f0393e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fc1f977abce4edc914a86203fec425b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c13b396bd774f24b5dba4fcf4e533f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cafec088deaa4044acecbff44d11c516": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a1a3ac56b5345a19c79e6afe378ee38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install petals\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "HV9nkxbWIsx9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dffd266-8560-494c-cbb7-49617ac5a3fc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: petals in /usr/local/lib/python3.10/dist-packages (2.0.0.post1)\n",
            "Requirement already satisfied: torch>=1.12 in /usr/local/lib/python3.10/dist-packages (from petals) (2.0.1+cu118)\n",
            "Requirement already satisfied: bitsandbytes==0.40.1.post1 in /usr/local/lib/python3.10/dist-packages (from petals) (0.40.1.post1)\n",
            "Requirement already satisfied: accelerate<0.21.0,>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from petals) (0.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from petals) (0.16.4)\n",
            "Requirement already satisfied: tokenizers>=0.13.3 in /usr/local/lib/python3.10/dist-packages (from petals) (0.13.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from petals) (4.31.0)\n",
            "Requirement already satisfied: speedtest-cli==2.1.3 in /usr/local/lib/python3.10/dist-packages (from petals) (2.1.3)\n",
            "Requirement already satisfied: pydantic<2.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from petals) (1.10.11)\n",
            "Requirement already satisfied: hivemind==1.1.8 in /usr/local/lib/python3.10/dist-packages (from petals) (1.1.8)\n",
            "Requirement already satisfied: tensor-parallel==1.0.23 in /usr/local/lib/python3.10/dist-packages (from petals) (1.0.23)\n",
            "Requirement already satisfied: humanfriendly in /usr/local/lib/python3.10/dist-packages (from petals) (10.0)\n",
            "Requirement already satisfied: async-timeout>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from petals) (4.0.2)\n",
            "Requirement already satisfied: cpufeature>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from petals) (0.2.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from petals) (23.1)\n",
            "Requirement already satisfied: sentencepiece>=0.1.99 in /usr/local/lib/python3.10/dist-packages (from petals) (0.1.99)\n",
            "Requirement already satisfied: peft>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from petals) (0.4.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from petals) (0.3.1)\n",
            "Requirement already satisfied: Dijkstar>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from petals) (2.6.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from hivemind==1.1.8->petals) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.1.8->petals) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.1.8->petals) (1.10.1)\n",
            "Requirement already satisfied: prefetch-generator>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.1.8->petals) (1.0.3)\n",
            "Requirement already satisfied: msgpack>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.1.8->petals) (1.0.5)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from hivemind==1.1.8->petals) (2.4.0)\n",
            "Requirement already satisfied: uvloop>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.1.8->petals) (0.17.0)\n",
            "Requirement already satisfied: grpcio-tools>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.1.8->petals) (1.48.2)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.1.8->petals) (3.20.3)\n",
            "Requirement already satisfied: configargparse>=1.2.3 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.1.8->petals) (1.5.5)\n",
            "Requirement already satisfied: multiaddr>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.1.8->petals) (0.0.9)\n",
            "Requirement already satisfied: pymultihash>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.1.8->petals) (0.8.2)\n",
            "Requirement already satisfied: cryptography>=3.4.6 in /usr/lib/python3/dist-packages (from hivemind==1.1.8->petals) (3.4.8)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<0.21.0,>=0.20.3->petals) (5.9.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from Dijkstar>=2.6.0->petals) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.1->petals) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.1->petals) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.1->petals) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.1->petals) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.1->petals) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->petals) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->petals) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->petals) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->petals) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.12->petals) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.12->petals) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.31.0->petals) (2022.10.31)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.33.2->hivemind==1.1.8->petals) (1.56.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.33.2->hivemind==1.1.8->petals) (67.7.2)\n",
            "Requirement already satisfied: varint in /usr/local/lib/python3.10/dist-packages (from multiaddr>=0.0.9->hivemind==1.1.8->petals) (1.0.2)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.10/dist-packages (from multiaddr>=0.0.9->hivemind==1.1.8->petals) (2.1.1)\n",
            "Requirement already satisfied: netaddr in /usr/local/lib/python3.10/dist-packages (from multiaddr>=0.0.9->hivemind==1.1.8->petals) (0.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12->petals) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0.0,>=0.11.1->petals) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0.0,>=0.11.1->petals) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0.0,>=0.11.1->petals) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0.0,>=0.11.1->petals) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12->petals) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from datasets import load_dataset, concatenate_datasets\n"
      ],
      "metadata": {
        "id": "ZfPzYu14GUgA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"sciq\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "de4faf6e739549b39de4713fff901147",
            "86d680cc08c84d9f84aa7dcba6b65daa",
            "4958b7adafd3427486dbf2c944e257fd",
            "5706775750cf41878e3674389abf9758",
            "29208aa6a7c24da598437704d45775e7",
            "98a95d387c434bde94d31e85d4ba797a",
            "b571a882de14415c97ea703e0f0393e5",
            "6fc1f977abce4edc914a86203fec425b",
            "1c13b396bd774f24b5dba4fcf4e533f9",
            "cafec088deaa4044acecbff44d11c516",
            "6a1a3ac56b5345a19c79e6afe378ee38"
          ]
        },
        "id": "PSDknLaZIYgh",
        "outputId": "18c86637-6ff8-4553-bb0c-435042afb9fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset sciq (/root/.cache/huggingface/datasets/sciq/default/0.1.0/50e5c6e3795b55463819d399ec417bfd4c3c621105e00295ddb5f3633d708493)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de4faf6e739549b39de4713fff901147"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=concatenate_datasets([dataset['train'],dataset['validation'],dataset['test']])\n",
        "dataset=dataset.select(range(500))"
      ],
      "metadata": {
        "id": "2MsKXFD9KqgV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=dataset.remove_columns(set(dataset.column_names)-{'question','correct_answer'})\n",
        "dataset.column_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgTTi-_oIq1m",
        "outputId": "4d7c2202-c6c0-4140-e88b-d59345094cde"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['question', 'correct_answer']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.DataFrame(dataset)\n",
        "data.columns=['prompt','response']"
      ],
      "metadata": {
        "id": "eD7HQwz0GgS3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DblBg2tEQrVP",
        "outputId": "d99e7eca-4a2e-4242-9691-adc50be2797d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "prompt      0\n",
              "response    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nwYoFGVQFDe",
        "outputId": "b4365c2e-038a-4f04-fd74-7f9f70237c1e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['prompt']=data['prompt'].map(lambda x: \"Give me a concise and specific answer to the following question: \"+str(x))\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "oPMlzefQwhz8",
        "outputId": "3ed3b8e6-4336-4118-e808-2ce461c97972"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                prompt  \\\n",
              "0    Give me a concise and specific answer to the f...   \n",
              "1    Give me a concise and specific answer to the f...   \n",
              "2    Give me a concise and specific answer to the f...   \n",
              "3    Give me a concise and specific answer to the f...   \n",
              "4    Give me a concise and specific answer to the f...   \n",
              "..                                                 ...   \n",
              "495  Give me a concise and specific answer to the f...   \n",
              "496  Give me a concise and specific answer to the f...   \n",
              "497  Give me a concise and specific answer to the f...   \n",
              "498  Give me a concise and specific answer to the f...   \n",
              "499  Give me a concise and specific answer to the f...   \n",
              "\n",
              "                                   response  \n",
              "0                      mesophilic organisms  \n",
              "1                           coriolis effect  \n",
              "2                                exothermic  \n",
              "3                               alpha decay  \n",
              "4                             smoke and ash  \n",
              "..                                      ...  \n",
              "495                                   ozone  \n",
              "496  two ions need to have opposite charges  \n",
              "497                  comparative embryology  \n",
              "498                                   lungs  \n",
              "499                             adaptations  \n",
              "\n",
              "[500 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-0590dfa5-afc9-4b50-ae39-1e07b3d18546\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Give me a concise and specific answer to the f...</td>\n",
              "      <td>mesophilic organisms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Give me a concise and specific answer to the f...</td>\n",
              "      <td>coriolis effect</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Give me a concise and specific answer to the f...</td>\n",
              "      <td>exothermic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Give me a concise and specific answer to the f...</td>\n",
              "      <td>alpha decay</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Give me a concise and specific answer to the f...</td>\n",
              "      <td>smoke and ash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>Give me a concise and specific answer to the f...</td>\n",
              "      <td>ozone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>Give me a concise and specific answer to the f...</td>\n",
              "      <td>two ions need to have opposite charges</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>Give me a concise and specific answer to the f...</td>\n",
              "      <td>comparative embryology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>Give me a concise and specific answer to the f...</td>\n",
              "      <td>lungs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>Give me a concise and specific answer to the f...</td>\n",
              "      <td>adaptations</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0590dfa5-afc9-4b50-ae39-1e07b3d18546')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-455d2b0d-c9e5-4b74-a8c5-abd89ce744d4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-455d2b0d-c9e5-4b74-a8c5-abd89ce744d4')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-455d2b0d-c9e5-4b74-a8c5-abd89ce744d4 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0590dfa5-afc9-4b50-ae39-1e07b3d18546 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0590dfa5-afc9-4b50-ae39-1e07b3d18546');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.iloc[0]['prompt'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpF2QFXUyCKX",
        "outputId": "91a1f47f-adeb-4801-f2f0-918b195260ec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give me a concise and specific answer to the following question: What type of organism is commonly used in preparation of foods such as cheese and yogurt?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from petals import AutoDistributedModelForCausalLM\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model_name=\"enoch/llama-65b-hf\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "model = AutoDistributedModelForCausalLM.from_pretrained(model_name, tuning_mode='deep_ptune', pre_seq_len=3)\n",
        "model = model.cuda()\n",
        "\n",
        "\n",
        "class ScienceDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        prompt = self.df.iloc[idx]['prompt']\n",
        "        response = self.df.iloc[idx]['response']\n",
        "        inputs = self.tokenizer(prompt, return_tensors='pt', padding='max_length', max_length=self.max_length, truncation=True)\n",
        "        labels = self.tokenizer(response, return_tensors='pt', padding='max_length', max_length=self.max_length, truncation=True)['input_ids']\n",
        "        return inputs['input_ids'].squeeze(0), labels.squeeze(0)\n",
        "\n",
        "\n",
        "dataset = ScienceDataset(data, tokenizer,128)\n",
        "dataloader = DataLoader(dataset, batch_size=64)\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters(), 3e-5)\n",
        "\n",
        "\n",
        "\n",
        "for batch in dataloader:\n",
        "    input_ids,  labels = batch\n",
        "    input_ids = input_ids.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    loss = model(input_ids=input_ids, labels=labels).loss\n",
        "\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    print(\"opt.step()\")\n",
        "\n",
        "    print(f\"loss = {loss.item():.3f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzePYdq8AFER",
        "outputId": "aba784c0-413d-43ed-d21a-59886d5b3944"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
            "Jul 20 08:06:21.575 [\u001b[1m\u001b[34mINFO\u001b[0m] Make sure you follow the LLaMA's terms of use: https://bit.ly/llama2-license for LLaMA 2, https://bit.ly/llama-license for LLaMA 1\n",
            "Jul 20 08:06:21.577 [\u001b[1m\u001b[34mINFO\u001b[0m] Using DHT prefix: llama-65b-hf\n",
            "Jul 20 08:08:02.807 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.06541372589600762, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.043120335162078555, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.2080595669216823, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.1298647733592596, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.11804655420022755})) (retry in 0 sec): ControlFailure('Connect failed. msg=routing: not found')\n",
            "Jul 20 08:08:05.725 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.06541372589600762, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.043120335162078555, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.2080595669216823, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.1298647733592596, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.11804655420022755})) (retry in 0 sec): ControlFailure('Connect failed. msg=failed to find peers: routing: not found')\n",
            "Jul 20 08:08:06.252 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.06541372589600762, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.043484296929675974, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.2080595669216823, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.13004597228737175, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.11604007736007772, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.05588197099950776})) (retry in 1 sec): ControlFailure('Connect failed. msg=failed to find peers: routing: not found')\n",
            "Jul 20 08:08:35.532 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=3199.588996772061, public_name=None, version='2.0.0.post1', network_rps=3199.588996772061, forward_rps=1705405.2247371103, inference_rps=1445.9328046309631, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=188416, next_pings={'12D3KooWHUXmh37ZYu8KB4hpBYU6hsbPu2emfyjQJs92Gy6gCo8x': 0.23793598924649362, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.34596628906403903, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.21457514628649274, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWREsBjWgF9q6uRDkLYuHqaESYudDFU5GjmFCgzhD1Ewvv': 0.1857103604740813, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.25407033962354764, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.57271907499159, '12D3KooWNC2tcQduyiEkfuVncqMHhe9aBDFtWU5QJ65sXjEycmof': 0.22345993335824466, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.802842223160656, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.18911596899852157})) (retry in 1 sec): ControlFailure('Connect failed. msg=stream reset')\n",
            "Jul 20 08:08:35.534 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd)>, start=61, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=3199.588996772061, public_name=None, version='2.0.0.post1', network_rps=3199.588996772061, forward_rps=1705405.2247371103, inference_rps=1445.9328046309631, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=188416, next_pings={'12D3KooWHUXmh37ZYu8KB4hpBYU6hsbPu2emfyjQJs92Gy6gCo8x': 0.24932024330667563, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.34596628906403903, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.21457514628649274, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWREsBjWgF9q6uRDkLYuHqaESYudDFU5GjmFCgzhD1Ewvv': 0.1856778392396518, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.2244791762781097, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.57271907499159, '12D3KooWNC2tcQduyiEkfuVncqMHhe9aBDFtWU5QJ65sXjEycmof': 0.23075922119896863, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 1.0077295680018143})) (retry in 0 sec): ControlFailure('Connect failed. msg=stream reset')\n",
            "Jul 20 08:10:51.299 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_backward:176\u001b[0m] Caught exception when running backward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=61, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.06541372589600762, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.043120335162078555, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.2080595669216823, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.1298647733592596, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.11804655420022755})) (retry in 0 sec): P2PHandlerError('Failed to call handler `TransformerConnectionHandler.rpc_backward_stream` at 12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 14.75 GiB total capacity; 12.43 GiB already allocated; 206.81 MiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
            "Jul 20 08:10:51.357 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_backward:176\u001b[0m] Caught exception when running backward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.06541372589600762, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.043484296929675974, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.2080595669216823, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.13004597228737175, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.11604007736007772, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.05588197099950776})) (retry in 0 sec): P2PHandlerError('Failed to call handler `TransformerConnectionHandler.rpc_backward_stream` at 12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 14.75 GiB total capacity; 12.43 GiB already allocated; 206.81 MiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
            "Jul 20 08:10:51.431 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_backward:176\u001b[0m] Caught exception when running backward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=61, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.06541372589600762, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.043484296929675974, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.2080595669216823, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.13004597228737175, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.11604007736007772, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.05588197099950776})) (retry in 0 sec): P2PHandlerError('Failed to call handler `TransformerConnectionHandler.rpc_backward_stream` at 12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 14.75 GiB total capacity; 12.43 GiB already allocated; 206.81 MiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
            "Jul 20 08:11:06.899 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.06944491811668735, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.043367044743785624, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.1304056691439259, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.1344808950705524, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.05588197099950776, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf})) (retry in 0 sec): P2PHandlerError('Failed to call handler `TransformerConnectionHandler.rpc_forward_stream` at 12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 14.75 GiB total capacity; 12.45 GiB already allocated; 206.81 MiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
            "Jul 20 08:11:14.492 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=62, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.06944491811668735, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.043367044743785624, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.1304056691439259, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.1344808950705524, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.05588197099950776, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf})) (retry in 0 sec): P2PHandlerError('Failed to call handler `TransformerConnectionHandler.rpc_forward_stream` at 12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 14.75 GiB total capacity; 12.50 GiB already allocated; 206.81 MiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
            "Jul 20 08:11:30.600 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=62, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.06944491811668735, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.043367044743785624, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.1304056691439259, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.1344808950705524, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.05588197099950776, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf})) (retry in 1 sec): P2PHandlerError('Failed to call handler `TransformerConnectionHandler.rpc_forward_stream` at 12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 14.75 GiB total capacity; 12.50 GiB already allocated; 206.81 MiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
            "Jul 20 08:11:32.266 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd)>, start=62, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=3199.588996772061, public_name=None, version='2.0.0.post1', network_rps=3199.588996772061, forward_rps=1705405.2247371103, inference_rps=1445.9328046309631, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=188416, next_pings={'12D3KooWHUXmh37ZYu8KB4hpBYU6hsbPu2emfyjQJs92Gy6gCo8x': 0.23793598924649362, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.20764170682216934, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWREsBjWgF9q6uRDkLYuHqaESYudDFU5GjmFCgzhD1Ewvv': 0.1857103604740813, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.25407033962354764, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.57271907499159, '12D3KooWNC2tcQduyiEkfuVncqMHhe9aBDFtWU5QJ65sXjEycmof': 0.21729072028642993, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.6995140137286766, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.18849076539772797, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.4085569119997672})) (retry in 0 sec): ControlFailure('Connect failed. msg=stream reset')\n",
            "Jul 20 08:11:51.489 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd)>, start=61, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=3199.588996772061, public_name=None, version='2.0.0.post1', network_rps=3199.588996772061, forward_rps=1705405.2247371103, inference_rps=1445.9328046309631, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=188416, next_pings={'12D3KooWHUXmh37ZYu8KB4hpBYU6hsbPu2emfyjQJs92Gy6gCo8x': 0.23793598924649362, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.20764170682216934, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWREsBjWgF9q6uRDkLYuHqaESYudDFU5GjmFCgzhD1Ewvv': 0.1857103604740813, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.25407033962354764, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.57271907499159, '12D3KooWNC2tcQduyiEkfuVncqMHhe9aBDFtWU5QJ65sXjEycmof': 0.21729072028642993, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.6995140137286766, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.18849076539772797, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.4085569119997672})) (retry in 0 sec): ControlFailure('Connect failed. msg=context deadline exceeded')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "opt.step()\n",
            "loss = 7.860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Jul 20 08:14:03.566 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=3199.588996772061, public_name=None, version='2.0.0.post1', network_rps=3199.588996772061, forward_rps=1705405.2247371103, inference_rps=1445.9328046309631, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=188416, next_pings={'12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.20764170682216934, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWREsBjWgF9q6uRDkLYuHqaESYudDFU5GjmFCgzhD1Ewvv': 0.18585919878458657, '12D3KooWNC2tcQduyiEkfuVncqMHhe9aBDFtWU5QJ65sXjEycmof': 0.21729072028642993, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.5074670075966583, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.18771717597596585, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.4085569119997672, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.6406997011974453})) (retry in 0 sec): ControlFailure('Connect failed. msg=stream reset')\n",
            "Jul 20 08:15:22.386 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.07161581169330017, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.043198715594971404, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.1402611081151237, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.1344808950705524, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.11871420600073179})) (retry in 0 sec): P2PHandlerError('Failed to call handler `TransformerConnectionHandler.rpc_forward_stream` at 12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 14.75 GiB total capacity; 12.50 GiB already allocated; 206.81 MiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
            "Jul 20 08:15:40.225 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.07377412375473794, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.043103485275903575, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.13821643829202018, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.11871420600073179})) (retry in 1 sec): P2PHandlerError('Failed to call handler `TransformerConnectionHandler.rpc_forward_stream` at 12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 14.75 GiB total capacity; 12.50 GiB already allocated; 206.81 MiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "opt.step()\n",
            "loss = 6.906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Jul 20 08:18:21.549 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd)>, start=40, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=3199.588996772061, public_name=None, version='2.0.0.post1', network_rps=3199.588996772061, forward_rps=1705405.2247371103, inference_rps=1445.9328046309631, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=188416, next_pings={'12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.20257639445788692, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWREsBjWgF9q6uRDkLYuHqaESYudDFU5GjmFCgzhD1Ewvv': 0.1857747113418206, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.4621906740774412, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.18743454138082458, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.577083130357787, '12D3KooWA8rYqs2SbC5yAHZBr4PqZR4dFgg1idUj54VqGeShGe7d': 0.6897807776794072, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.3725254560049507, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWNC2tcQduyiEkfuVncqMHhe9aBDFtWU5QJ65sXjEycmof': 0.20509880899771815})) (retry in 0 sec): ControlFailure('Connect failed. msg=stream reset')\n",
            "Jul 20 08:18:29.258 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.07377412375473794, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.043103485275903575, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.1616580694336227, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.11871420600073179, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.2722641999998814, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.3850527360000342})) (retry in 0 sec): P2PHandlerError('Failed to call handler `TransformerConnectionHandler.rpc_forward_stream` at 12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 14.75 GiB total capacity; 12.50 GiB already allocated; 206.81 MiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
            "Jul 20 08:18:40.773 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=3199.588996772061, public_name=None, version='2.0.0.post1', network_rps=3199.588996772061, forward_rps=1705405.2247371103, inference_rps=1445.9328046309631, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=188416, next_pings={'12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.20257639445788692, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWREsBjWgF9q6uRDkLYuHqaESYudDFU5GjmFCgzhD1Ewvv': 0.1857747113418206, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.4621906740774412, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.18743454138082458, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.577083130357787, '12D3KooWA8rYqs2SbC5yAHZBr4PqZR4dFgg1idUj54VqGeShGe7d': 0.6897807776794072, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.3725254560049507, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWNC2tcQduyiEkfuVncqMHhe9aBDFtWU5QJ65sXjEycmof': 0.20509880899771815})) (retry in 0 sec): ControlFailure('Connect failed. msg=context deadline exceeded')\n",
            "Jul 20 08:19:17.966 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.07377412375473794, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.043103485275903575, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.1616580694336227, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.11871420600073179, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.2722641999998814, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.3850527360000342})) (retry in 0 sec): P2PHandlerError('Failed to call handler `TransformerConnectionHandler.rpc_forward_stream` at 12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 14.75 GiB total capacity; 12.50 GiB already allocated; 206.81 MiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "opt.step()\n",
            "loss = 7.087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Jul 20 08:23:26.528 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd)>, start=40, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=3199.588996772061, public_name=None, version='2.0.0.post1', network_rps=3199.588996772061, forward_rps=1705405.2247371103, inference_rps=1445.9328046309631, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=188416, next_pings={'12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.19895373229309005, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.42608014706185643, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.1869493333637822, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.5996591610300006, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.32385060715241126, '12D3KooWNC2tcQduyiEkfuVncqMHhe9aBDFtWU5QJ65sXjEycmof': 0.2695496565595386, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.28918945300392807, '12D3KooWN82Cdn1uLLufLnKM2Az1pXQeGkdD8rrDoQUFFfDgvVPs': inf, '12D3KooWREsBjWgF9q6uRDkLYuHqaESYudDFU5GjmFCgzhD1Ewvv': 0.18557256220519777, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf})) (retry in 0 sec): ControlFailure('Connect failed. msg=stream reset')\n",
            "Jul 20 08:23:26.534 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=3199.588996772061, public_name=None, version='2.0.0.post1', network_rps=3199.588996772061, forward_rps=1705405.2247371103, inference_rps=1445.9328046309631, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=188416, next_pings={'12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.19895373229309005, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.42608014706185643, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.1869493333637822, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.5996591610300006, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.32385060715241126, '12D3KooWNC2tcQduyiEkfuVncqMHhe9aBDFtWU5QJ65sXjEycmof': 0.2695496565595386, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.28918945300392807, '12D3KooWN82Cdn1uLLufLnKM2Az1pXQeGkdD8rrDoQUFFfDgvVPs': inf, '12D3KooWREsBjWgF9q6uRDkLYuHqaESYudDFU5GjmFCgzhD1Ewvv': 0.18557256220519777, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf})) (retry in 0 sec): ControlFailure('Connect failed. msg=stream reset')\n",
            "Jul 20 08:23:26.539 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd)>, start=40, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=3199.588996772061, public_name=None, version='2.0.0.post1', network_rps=3199.588996772061, forward_rps=1705405.2247371103, inference_rps=1445.9328046309631, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=188416, next_pings={'12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.19895373229309005, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.42608014706185643, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.1869493333637822, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.5996591610300006, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.32385060715241126, '12D3KooWNC2tcQduyiEkfuVncqMHhe9aBDFtWU5QJ65sXjEycmof': 0.2695496565595386, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.28918945300392807, '12D3KooWN82Cdn1uLLufLnKM2Az1pXQeGkdD8rrDoQUFFfDgvVPs': inf, '12D3KooWREsBjWgF9q6uRDkLYuHqaESYudDFU5GjmFCgzhD1Ewvv': 0.18557256220519777, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf})) (retry in 0 sec): ControlFailure('Connect failed. msg=stream reset')\n",
            "Jul 20 08:24:07.542 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=3199.588996772061, public_name=None, version='2.0.0.post1', network_rps=3199.588996772061, forward_rps=1705405.2247371103, inference_rps=1445.9328046309631, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=188416, next_pings={'12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.19895373229309005, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.3972236006499001, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.5996591610300006, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.32385060715241126, '12D3KooWNC2tcQduyiEkfuVncqMHhe9aBDFtWU5QJ65sXjEycmof': 0.2695496565595386, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWN82Cdn1uLLufLnKM2Az1pXQeGkdD8rrDoQUFFfDgvVPs': inf, '12D3KooWREsBjWgF9q6uRDkLYuHqaESYudDFU5GjmFCgzhD1Ewvv': 0.18557256220519777, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.18622068999684416})) (retry in 0 sec): ControlFailure('Connect failed. msg=stream reset')\n",
            "Jul 20 08:24:26.767 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=3199.588996772061, public_name=None, version='2.0.0.post1', network_rps=3199.588996772061, forward_rps=1705405.2247371103, inference_rps=1445.9328046309631, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=188416, next_pings={'12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.19895373229309005, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.3972236006499001, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.5996591610300006, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.32385060715241126, '12D3KooWNC2tcQduyiEkfuVncqMHhe9aBDFtWU5QJ65sXjEycmof': 0.2695496565595386, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWN82Cdn1uLLufLnKM2Az1pXQeGkdD8rrDoQUFFfDgvVPs': inf, '12D3KooWREsBjWgF9q6uRDkLYuHqaESYudDFU5GjmFCgzhD1Ewvv': 0.18557256220519777, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.18622068999684416})) (retry in 1 sec): ControlFailure('Connect failed. msg=context deadline exceeded')\n",
            "Jul 20 08:24:26.904 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=3199.588996772061, public_name=None, version='2.0.0.post1', network_rps=3199.588996772061, forward_rps=1705405.2247371103, inference_rps=1445.9328046309631, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=188416, next_pings={'12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.19895373229309005, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.42608014706185643, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.1869493333637822, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.5996591610300006, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.32385060715241126, '12D3KooWNC2tcQduyiEkfuVncqMHhe9aBDFtWU5QJ65sXjEycmof': 0.2695496565595386, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.28918945300392807, '12D3KooWN82Cdn1uLLufLnKM2Az1pXQeGkdD8rrDoQUFFfDgvVPs': inf, '12D3KooWREsBjWgF9q6uRDkLYuHqaESYudDFU5GjmFCgzhD1Ewvv': 0.18557256220519777, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf})) (retry in 0 sec): ControlFailure('Connect failed. msg=context deadline exceeded')\n",
            "Jul 20 08:24:48.987 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.07236123137032321, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.06846832579625849, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.14982785065416568, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.2604428124865473, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.2714848349996828, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.07285624100040877})) (retry in 2 sec): P2PHandlerError('Failed to call handler `TransformerConnectionHandler.rpc_forward_stream` at 12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 14.75 GiB total capacity; 12.50 GiB already allocated; 206.81 MiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "opt.step()\n",
            "loss = 7.070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Jul 20 08:28:13.473 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=709.5161473280316, public_name=None, version='1.2.0.dev3', network_rps=709.5161473280316, forward_rps=37312.611700620575, inference_rps=419.00907164767057, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=False, cache_tokens_left=163840, next_pings={'12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.25590827291194373, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.27018776880856965, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.03899554543601991, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.038169225528836254, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.22425006777048112})) (retry in 0 sec): ControlFailure('Connect failed. msg=protocol not supported')\n",
            "Jul 20 08:28:13.550 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=709.5161473280316, public_name=None, version='1.2.0.dev3', network_rps=709.5161473280316, forward_rps=37312.611700620575, inference_rps=419.00907164767057, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=False, cache_tokens_left=163840, next_pings={'12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.27018776880856965, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.03899554543601991, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.038647074878215795, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.22425006777048112})) (retry in 1 sec): ControlFailure('Connect failed. msg=protocol not supported')\n",
            "Jul 20 08:28:14.668 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=709.5161473280316, public_name=None, version='1.2.0.dev3', network_rps=709.5161473280316, forward_rps=37312.611700620575, inference_rps=419.00907164767057, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=False, cache_tokens_left=163840, next_pings={'12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.27018776880856965, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.03899554543601991, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.038647074878215795, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.22425006777048112})) (retry in 2 sec): ControlFailure('Connect failed. msg=protocol not supported')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "opt.step()\n",
            "loss = 6.907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Jul 20 08:31:14.160 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.07016981866347391, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.1408073555669488, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.2298856125892731, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.24261681869575114, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.08154137619967515, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.08449907100020937})) (retry in 0 sec): P2PHandlerError('Failed to call handler `TransformerConnectionHandler.rpc_forward_stream` at 12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 14.75 GiB total capacity; 12.47 GiB already allocated; 206.81 MiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
            "Jul 20 08:31:17.153 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.07016981866347391, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.1408073555669488, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.2298856125892731, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.24261681869575114, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.08154137619967515, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.08449907100020937})) (retry in 1 sec): P2PHandlerError('Failed to call handler `TransformerConnectionHandler.rpc_forward_stream` at 12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 14.75 GiB total capacity; 12.47 GiB already allocated; 206.81 MiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
            "Jul 20 08:31:17.821 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd)>, start=40, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=3199.588996772061, public_name=None, version='2.0.0.post1', network_rps=3199.588996772061, forward_rps=1705405.2247371103, inference_rps=1445.9328046309631, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=188416, next_pings={'12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.3407348677564252, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.6139822430197885, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.33356321192142324, '12D3KooWREsBjWgF9q6uRDkLYuHqaESYudDFU5GjmFCgzhD1Ewvv': 0.18554344852207, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWN82Cdn1uLLufLnKM2Az1pXQeGkdD8rrDoQUFFfDgvVPs': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.5473603249195149, '12D3KooWA8rYqs2SbC5yAHZBr4PqZR4dFgg1idUj54VqGeShGe7d': 1.0924815492035123, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.27038937540201, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf})) (retry in 0 sec): ControlFailure('Connect failed. msg=stream reset')\n",
            "Jul 20 08:31:21.930 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.07016981866347391, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.1408073555669488, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.2298856125892731, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.24261681869575114, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.08154137619967515, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.08449907100020937})) (retry in 2 sec): P2PHandlerError('Failed to call handler `TransformerConnectionHandler.rpc_forward_stream` at 12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 14.75 GiB total capacity; 12.47 GiB already allocated; 206.81 MiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
            "Jul 20 08:31:27.523 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.07016981866347391, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.1408073555669488, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.2298856125892731, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.24261681869575114, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.08154137619967515, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.08449907100020937})) (retry in 4 sec): P2PHandlerError('Failed to call handler `TransformerConnectionHandler.rpc_forward_stream` at 12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 14.75 GiB total capacity; 12.47 GiB already allocated; 206.81 MiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
            "Jul 20 08:32:04.093 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.07016981866347391, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.1408073555669488, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.2298856125892731, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.24261681869575114, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.08154137619967515, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.08449907100020937})) (retry in 0 sec): P2PHandlerError('Failed to call handler `TransformerConnectionHandler.rpc_forward_stream` at 12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 14.75 GiB total capacity; 12.50 GiB already allocated; 206.81 MiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
            "Jul 20 08:32:44.380 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.07016981866347391, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.1408073555669488, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.2298856125892731, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.24261681869575114, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.08154137619967515, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.08449907100020937})) (retry in 0 sec): P2PHandlerError('Failed to call handler `TransformerConnectionHandler.rpc_forward_stream` at 12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 14.75 GiB total capacity; 12.50 GiB already allocated; 206.81 MiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
            "Jul 20 08:32:51.934 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.07016981866347391, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.1408073555669488, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.2298856125892731, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.24261681869575114, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.08154137619967515, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.08449907100020937})) (retry in 0 sec): P2PHandlerError('Failed to call handler `TransformerConnectionHandler.rpc_forward_stream` at 12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 14.75 GiB total capacity; 12.50 GiB already allocated; 206.81 MiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
            "Jul 20 08:32:57.119 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=3199.588996772061, public_name=None, version='2.0.0.post1', network_rps=3199.588996772061, forward_rps=1705405.2247371103, inference_rps=1445.9328046309631, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=188416, next_pings={'12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.3407348677564252, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.6139822430197885, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.33356321192142324, '12D3KooWREsBjWgF9q6uRDkLYuHqaESYudDFU5GjmFCgzhD1Ewvv': 0.18554344852207, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWN82Cdn1uLLufLnKM2Az1pXQeGkdD8rrDoQUFFfDgvVPs': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.5473603249195149, '12D3KooWA8rYqs2SbC5yAHZBr4PqZR4dFgg1idUj54VqGeShGe7d': 1.0924815492035123, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.27038937540201, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf})) (retry in 0 sec): ControlFailure('Connect failed. msg=context deadline exceeded')\n",
            "Jul 20 08:33:01.817 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.07016981866347391, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.1408073555669488, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.2298856125892731, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.24261681869575114, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.08154137619967515, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.08449907100020937})) (retry in 0 sec): P2PHandlerError('Failed to call handler `TransformerConnectionHandler.rpc_forward_stream` at 12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 14.75 GiB total capacity; 12.50 GiB already allocated; 206.81 MiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "opt.step()\n",
            "loss = 7.053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Jul 20 08:40:09.271 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_backward:176\u001b[0m] Caught exception when running backward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=343.26418438019596, public_name='http://michaelkammes.com', version='2.0.0.post1', network_rps=343.26418438019596, forward_rps=324313.0493776356, inference_rps=435.8476915955684, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=90112, next_pings={'12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.33770354577532874, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.13452703817900927, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.10497445857468793, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.3213313337564632, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.13799902351031781, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.31994864199805306})) (retry in 0 sec): ValueError('not enough values to unpack (expected at least 1, got 0)')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "opt.step()\n",
            "loss = 6.658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Jul 20 08:44:50.128 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd)>, start=40, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=3199.588996772061, public_name=None, version='2.0.0.post1', network_rps=3199.588996772061, forward_rps=1705405.2247371103, inference_rps=1445.9328046309631, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=188416, next_pings={'12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.43045059517291423, '12D3KooWA8rYqs2SbC5yAHZBr4PqZR4dFgg1idUj54VqGeShGe7d': 1.826019463800185, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.3270611594983661, '12D3KooWHUXmh37ZYu8KB4hpBYU6hsbPu2emfyjQJs92Gy6gCo8x': 0.19797052175970753, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.32195570972166027, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.5743143025974861, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.2813548806778272, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWREsBjWgF9q6uRDkLYuHqaESYudDFU5GjmFCgzhD1Ewvv': 0.18537189539783866, '12D3KooWN82Cdn1uLLufLnKM2Az1pXQeGkdD8rrDoQUFFfDgvVPs': inf})) (retry in 0 sec): ControlFailure('Connect failed. msg=context deadline exceeded')\n",
            "Jul 20 08:47:01.260 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=65, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.1349496904884552, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.2655396134887916, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.07484327539471354, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.043013139599497666, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.21122262999961094})) (retry in 0 sec): P2PHandlerError('Failed to call handler `TransformerConnectionHandler.rpc_forward_stream` at 12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 14.75 GiB total capacity; 12.51 GiB already allocated; 204.81 MiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
            "Jul 20 08:48:06.830 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K)>, start=65, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=4548.864555031427, public_name=None, version='2.0.0.post1', network_rps=4548.864555031427, forward_rps=417297.91126015485, inference_rps=207.13909703531036, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=114688, next_pings={'12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV': inf, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.24628436135277804, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWDsAtwTpuFcTxNEqQ6gPS6Co94X3tAVBz8ouk1WwWraTm': 0.11324748973239414, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.08543666056314542, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.21122262999961094, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.09469863460017222, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf})) (retry in 1 sec): P2PHandlerError('Failed to call handler `TransformerConnectionHandler.rpc_forward_stream` at 12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 14.75 GiB total capacity; 12.51 GiB already allocated; 204.81 MiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n",
            "Jul 20 08:48:11.716 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_backward:176\u001b[0m] Caught exception when running backward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY)>, start=60, end=80, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=2235.7226389647903, public_name='https://www.genesiscloud.com/', version='2.0.0.post1', network_rps=2235.7226389647903, forward_rps=253598.4983006539, inference_rps=485.43750353302534, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=81920, next_pings={'12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.21309968133247656, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.4862329367840511, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.08767414043564173, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.08340299539255128, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf})) (retry in 0 sec): ControlFailure('Connect failed. msg=protocol not supported')\n",
            "Jul 20 08:48:12.783 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_forward:99\u001b[0m] Caught exception when running forward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWMtENSTvyATqWQr5qT98QPznejak5tpts9NuFk8R5LubV)>, start=60, end=61, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=221.7465474136925, public_name=None, version='2.0.0.post1', network_rps=450.8663123815241, forward_rps=221.7465474136925, inference_rps=18.77803117741157, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=4096, next_pings={'12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.11454260025032761, '12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5': inf, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.1153181576728821, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.051615446843206896, '12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.08900543073564768, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf})) (retry in 0 sec): ControlFailure('Connect failed. msg=protocol not supported')\n",
            "Jul 20 08:49:15.063 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.sequential_autograd.sequential_backward:176\u001b[0m] Caught exception when running backward via RemoteSpanInfo(peer_id=<libp2p.peer.id.ID (12D3KooWADE41EXTAPu3fKT4QQ9Bfyb2oKgNy17hPR7atv1MJvh5)>, start=60, end=65, server_info=ServerInfo(state=<ServerState.ONLINE: 2>, throughput=713.0025378508676, public_name=None, version='1.2.0.dev4', network_rps=713.0025378508676, forward_rps=542590.6648413156, inference_rps=74.09810052035692, adapters=('timdettmers/guanaco-65b',), torch_dtype='float16', quant_type='nf4', using_relay=True, cache_tokens_left=20480, next_pings={'12D3KooWKJvVBGa2wkzyvmCD7WeGynexPCho8emgcd7HPk4HQeGd': inf, '12D3KooWEroZqPrvnaAGKCJvsPA3RC69wZ5vh6ktjN8ospYezmMc': 0.048328513017052384, '12D3KooWE2jTj6L2tjhYcZLfAAkER9ns2JT2F5mp81b4Kn33a5gy': 0.12759660432494938, '12D3KooWAZpDJGfcNKyWLJUmuaRfTB54Mf51a8DCiujYTcNMC929': 0.11677923469254202, '12D3KooWKvy8sj3vhPT8Y6LWYoDXP3CnENFqESozJnCD65Hn9RWf': 0.06368713494385665, '12D3KooWQMAxYKSiSJG2v8UA9CNbXnF1QqcTFNvSttz4oDWUfU7K': inf, '12D3KooWG36YZfwnAkiXcu5xBY86EHyD1S2Y2uBfNf9obK1GwPar': 0.20127680833966005, '12D3KooWEEvDW234YEPrqqixvtAheUcX1XUB7qbUG4pDBHjF4cGd': inf, '12D3KooWSxsRrX3e56SyDzHE5vZDBF27a4uB54jeyUcTyPBhmZxV': inf, '12D3KooWMVzqkaDLqroKhVawrAhTxwmJGWP6FPFF9Gu2P1NNZJCY': inf})) (retry in 0 sec): ValueError('not enough values to unpack (expected at least 1, got 0)')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "opt.step()\n",
            "loss = 6.592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"How do airplanes fly?\", return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
        "outputs = model.generate(inputs, max_new_tokens=15)\n",
        "print(\"generated:\", tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "shleC0-w0BEr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94564746-6236-47c6-8bc8-27aa54efc545"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Jul 20 08:55:03.891 [\u001b[1m\u001b[34mINFO\u001b[0m] Route found: 0:40 via …D1Ewvv => 40:80 via …NMC929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated: <s> How do airplanes fly?\n",
            "Airplanes fly by using the lift generated by their wings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"Why is the sky blue? Be consise and give straight answer\", return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
        "outputs = model.generate(inputs, max_new_tokens=50)\n",
        "print(\"generated:\", tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "zOVO1CzWqsQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fbb9200-2b95-4742-97b4-b0b0c6d9e37d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Jul 20 08:58:42.721 [\u001b[1m\u001b[34mINFO\u001b[0m] Route found: 0:40 via …D1Ewvv => 40:80 via …NMC929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated: <s> Why is the sky blue? Be consise and give straight answer.\n",
            "Asked by: Ajay\n",
            "The sky is blue because of the way the atmosphere scatters light. The atmosphere is made up of molecules of gas, and light hitting these molecules can be scattered in various directions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"What protects the lungs?  Be consise and give straight answer\", return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
        "outputs = model.generate(inputs, max_new_tokens=50)\n",
        "print(\"generated:\", tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmU0fFwxhLZb",
        "outputId": "e482a748-f2e4-4371-b11b-a012a7cc3bcd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Jul 20 08:59:22.698 [\u001b[1m\u001b[34mINFO\u001b[0m] Route found: 0:40 via …D1Ewvv => 40:80 via …NMC929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated: <s> What protects the lungs?  Be consise and give straight answer.\n",
            "The lungs are protected by the rib cage.\n",
            "The lungs are protected by the rib cage. The rib cage is a bony structure that protects the lungs.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from transformers import AutoTokenizer\n",
        "# from petals import AutoDistributedModelForCausalLM\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# model_name=\"enoch/llama-65b-hf\"\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# model = AutoDistributedModelForCausalLM.from_pretrained(model_name)\n",
        "# model = model.cuda()\n",
        "\n",
        "# class LLMBasedGenerator(nn.Module):\n",
        "#     def __init__(self, model):\n",
        "#         super().__init__()\n",
        "#         self.distributed_layers = model.transformer.h\n",
        "#         self.adapter = nn.Sequential(nn.Linear(model.config.hidden_size, 64), nn.Linear(64, model.config.hidden_size))\n",
        "#         self.head = model.lm_head\n",
        "\n",
        "#     def forward(self, embeddings):\n",
        "\n",
        "#         mid_block = len(self.distributed_layers) // 2\n",
        "#         hidden_states = self.distributed_layers[:mid_block](embeddings)\n",
        "\n",
        "#         hidden_states = hidden_states.to(self.adapter[0].weight.dtype)\n",
        "\n",
        "\n",
        "#         hidden_states = self.adapter(hidden_states)\n",
        "#         hidden_states = self.distributed_layers[mid_block:](hidden_states)\n",
        "#         return self.head(hidden_states)\n",
        "\n",
        "# generator = LLMBasedGenerator(model).cuda()\n",
        "\n",
        "# class ScienceDataset(Dataset):\n",
        "#     def __init__(self, df, tokenizer, max_length):\n",
        "#         self.df = df\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.max_length = max_length\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.df)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         prompt = self.df.iloc[idx]['prompt']\n",
        "#         response = self.df.iloc[idx]['response']\n",
        "#         inputs = self.tokenizer(prompt, return_tensors='pt', padding='max_length', max_length=self.max_length, truncation=True)\n",
        "#         labels = self.tokenizer(response, return_tensors='pt', padding='max_length', max_length=self.max_length, truncation=True)['input_ids']\n",
        "#         return inputs['input_ids'].squeeze(0), labels.squeeze(0)\n",
        "\n",
        "\n",
        "# dataset = ScienceDataset(data, tokenizer,256)\n",
        "# dataloader = DataLoader(dataset, batch_size=8)\n",
        "\n",
        "# opt = torch.optim.Adam(model.parameters(), 3e-5)\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# for epoch in range(3):\n",
        "#     for batch in dataloader:\n",
        "#         input_ids,  labels = batch\n",
        "#         input_ids = input_ids.cuda()\n",
        "#         labels = labels.cuda()\n",
        "\n",
        "#         input_ids=model.transformer.word_embeddings(input_ids)\n",
        "#         outputs = generator(input_ids)\n",
        "#         loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
        "#         print(f\"loss = {loss.item():.3f}\")\n",
        "#         opt.zero_grad()\n",
        "#         loss.backward()\n",
        "#         opt.step()\n",
        "\n",
        "# # print('predicted:', generator(inputs).argmax(-1))  #\n"
      ],
      "metadata": {
        "id": "NnvTegqUGIWR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P26pwswLhmoz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}